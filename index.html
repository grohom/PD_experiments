<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Prisoner's Dilemma Strategies</title>
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <h1>Prisoner's Dilemma Strategies</h1>
    
    <div id="content">

        <div id="simulation">
            <div id="graph-group">
                <img id="bgnd" src="graphs.svg">
                <div id="graph"></div>
            </div>
            <div id="controls">
                <div id="params1">
                    Live settings:
                    <br>
                    <input type="range" id="num-interactions" min="100" max="10000" step="100" value="500">
                    <label for="num-interactions">update display every <span class="ninteractions-text">500</span> games</label>
                    <br>
                    <input type="range" id="kill-fraction" min="0.001" max="0.02" step="0.001" value="0.003">
                    <label for="kill-fraction"><span class="fraction-text">0.3</span>% fight probability</label>
                </div>
                <div id="params2">
                    <button id="restart-button">Restart</button> with seed
                    <input type="text" id="random-seed" value="42">
                    <br>
                    <input type="checkbox" id="randomize-seed" checked="checked">
                    <label for="randomize-seed">randomize seed on restart</label>
                    <br>
                    <input type="range" id="num-agents" min="10" max="200" step="10" value="100">
                    <label for="num-agents"><span class="nagents-text">100</span> agents</label>
                    <br>
                    <input type="text" id="learn-min" value="0.33"> to
                    <input type="text" id="learn-max" value="0.66"> learn
                    <br>
                    <input type="text" id="p0-min" value="0.33"> to
                    <input type="text" id="p0-max" value="0.66"> p0
                </div>
            </div>
            </div>
        </div>

        <div id="explanation">

            <h2>Setting</h2>

            <ul>
            <li>there are <span class="nagents-text">100</span> agents</li>
            <li>each timestep, two of them are chosen randomly to play Prisoner's Dilemma:</li>
            <ul>
                <li>if one cooperates and the other defects, the cooperator gets 0 points and the defector gets 3 points</li>
                <li>if both cooperate, each gets 2 points</li>
                <li>if both defect, each gets 1 point</li>
            </ul>
            <li>the actions are encoded as <code>cooperation = 1</code>, <code>defection = 0</code><br>
            <li>an agent learns to predict the actions of each opponent it encounters:<br>
                <code>prediction += learn*(opponents_last_observed_action - prediction)</code>
            </li>
            <ul>
                <li>if <code>learn = 0</code>, the agent never changes its prediction</li>
                <li>if <code>learn = 1</code>, the prediction equals the opponent's last action</li>
                <li>if <code>0 &lt; learn &lt; 1</code>, the agent's prediction is a weighted average of the opponent's past actions (the weights decaying exponentially towards the past)</li>
            </ul>
            <li>when playing the first time with a new opponent, the initial prediction is a constant <code>p0</code> specific to each agent (inherited genetically)</li>
            <li>having a prediction <code>p</code> of the opponent's action, an agent acts according to that prediction: it cooperates with probability <code>p</code> and defects with probability <code>1 - p</code></li>
            <li>an agent's fitness is <code>fitness = points_won/games_played</code></li>
            <li>with probability <span class="fraction-text">0.3</span>%, the two agents engage in a fight instead of playing the game (<a href="https://en.wikipedia.org/wiki/Moran_process">Moran Process</a></li>
            <li>during the fight the fittest one kills the other and reproduces in its place</li>
            <li>the child inherits (with mutation) two parameters: <code>learn</code> and <code>p0</code> (whatever the parent has learned is not passed down)</li>
            </ul>

            <h2>Graphs</h2>

            <p>After <span class="ninteractions-text">500</span> games, agents are plotted on the two graphs.</p>

            <p>The rightmost graph contains pairs <code>(learn, p0)</code>:</p>
            <ul>
                <li>the left side = agents dont't learn (they have "strong preconceptions")</li>
                <li>the right side = agents fully embrace any new information (the opponent's last action), completely forgetting the past</li>
                <li>the bottom side = agents are distrustful (before starting to learn, they predict that the opponent will defect)</li>
                <li>the top side = agents are trustful (before starting to learn, they predict that the opponent will cooperate)</li>
            </ul>
            <p>The leftmost graph depicts pairs <code>(fitness, %cooperations)</code>:</p>
            <ul>
                <li>on the horizontal axis: the average number of points/game (min 0, max 3)</li>
                <li>on the vertical axis: the percentage of cooperations</li>
            </ul>

            <p>On both graphs, younger agents are paler.</p>

            <h2>Controls</h2>

            <p>Click somewhere in the left graph to generate an agent with that <code>(learn, p0)</code>.</p>

            <p>Or use the keyboard to generate "pure types":</p>

            <ul>
            <li><code>T</code> = Tit For Tat (start with cooperation, then copy the opponent's action)</li>
            <li><code>C</code> = Always Cooperate</li>
            <li><code>D</code> = Always Defect</li>
            <li><code>N</code> = Nasty Tit For Tat (start with defection, then copy the opponent's action)</li>
            </ul>

            <p>Parameters on the left can be changed live.</p>

            <p>The right box is for restarting the simulation (press <code>R</code> or use the button) â€“ there you can set the number of agents and the limits for initializing <code>learn</code> and <code>p0</code>.</p>

            <p>Each time you (re)start, a new random seed will be generated. If you want to keep the seed (to replay a certain evolution), deselect the "randomize" checkbox. You can also paste some seed you liked and saved earlier.</p>
            
        </div>

    </div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/p5.js/1.4.0/p5.js"></script>
<script src="sketch.js"></script>
</body>
</html>
