<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Prisoner's Dilemma Strategies</title>
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <h1>Prisoner's Dilemma Strategies</h1>
    
    <div id="content">

        <div id="simulation">
            <div id="graph"></div>
            <button id="restart-button">Restart</button>
            <div id="params">
            <input type="range" id="num-agents" min="10" max="200" step="10" value="100">
            <label for="num-agents">number of agents</label>
            <br>
            <input type="range" id="num-interactions" min="1000" max="10000" step="1000" value="5000">
            <label for="num-interactions">number of interactions</label>
            <br>
            <input type="range" id="kill-fraction" min="0.01" max="0.5" step="0.01" value="0.33">
            <label for="kill-fraction">fraction of population to kill</label>
            </div>
        </div>

        <div id="explanation">

            <h2>Setting</h2>

            <ul>
            <li>there are <span class="nagents_text">100</span> agents</li>
            <li>in each round they are paired randomly to play <span class="ninteractions_text">5000</span> Prisoner's Dilemmas:</li>
            <ul>
                <li>if one cooperates and the other defects, the cooperator gets 0 points and the defector gets 3 points</li>
                <li>if both cooperate, each gets 2 points</li>
                <li>if both defect, each gets 1 point</li>
            </ul>
            <li>the actions are encoded as <code>cooperation = 0</code>, <code>defection = 1</code><br>
            <li>an agent learns to predict the actions of each opponent it encounters:<br>
                <code>prediction += learn*(opponents_last_observed_action - prediction)</code>
            </li>
            <ul>
                <li>if <code>learn = 0</code>, the agent never changes its prediction</li>
                <li>if <code>learn = 1</code>, the prediction equals the opponent's last action</li>
                <li>if <code>0 &lt; learn &lt; 1</code>, the agent's prediction is a weighted average of the opponent's past actions (the weights decaying exponentially towards the past)</li>
            </ul>
            <li>when interacting the first time with a new opponent, the initial prediction equals the average prediction (over all previous interactions of that agent)</li>
            <li>if there are no previous interactions (newborn agent) the initial prediction is a random number <code>p0</code> inherited genetically</li>
            <li>having a prediction <code>p</code> of the opponent's action, an agent acts according to that prediction:</li>
            <ul>
                <li><span style="color:rgb(0, 200, 80)">normal</span> agents defect with probability <code>p</code> and cooperate with probability <code>1 - p</code></li>
                <li><span style="color:rgb(255, 100, 0)">reverse</span> agents cooperate with probability <code>p</code> and defect with probability <code>1 - p</code></li>
            </ul>
            <li>after <span class="ninteractions_text">5000</span> random interactions, the agents are sorted by how many points they gained per interaction</li>
            <li>the bottom <span class="fraction_text">33</span>% die, and the top <span class="fraction_text">33</span>% reproduce in their place</li>
            <li>when an agent reproduces, the child inherits (with mutation) two parameters: <code>learn</code> and <code>p0</code> (whatever the parent has learned is not passed down)</li>
            </ul>

            <h2>The graph</h2>

            <p>After each round of <span class="ninteractions_text">5000</span> interactions, the pairs <code>(learn, p0)</code> are plotted for all agents:</p>
            <ul>
                <li>the left edge = agents dont't learn (they have "strong preconceptions")</li>
                <li>the right edge = agents fully embrace any new information (the opponent's last action), forgetting the past</li>
                <li>the bottom edge = agents are born distrustful (before starting to learn, they predict that the opponent will defect)</li>
                <li>the top edge = agents are born trustful (before starting to learn, they predict that the opponent will cooperate)</li>
            </ul>
            <p>The vertical line shows the average payoff per interaction (the right edge = payoff 2, total cooperation).</p>
            <p>The horizontal line shows how many cooperations there are (up = cooperations, down = defections).</p>

            <h2>Keys/Mouse</h2>

            <p>Click somewhere in the graph to generate an agent with that <code>(learn, p0)</code>.</p>

            <p>Or use the keyboard to generate "pure types":</p>

            <ul>
            <li><code>T</code> = Tit For Tat (start with cooperation, then copy the opponent's action)</li>
            <li><code>C</code> = Always Cooperate</li>
            <li><code>D</code> = Always Defect</li>
            <li><code>N</code> = Nasty Tit For Tat (start with defection, then copy the opponent's action)</li>
            </ul>

        </div>

    </div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/p5.js/1.4.0/p5.js"></script>
<script src="sketch.js"></script>
</body>
</html>
